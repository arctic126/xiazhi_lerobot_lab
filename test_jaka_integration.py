#!/usr/bin/env python3
"""
JAKAæœºå™¨äººLeRoboté›†æˆæµ‹è¯•è„šæœ¬

æµ‹è¯•åŠŸèƒ½ï¼š
1. JAKAæœºå™¨äººè¿æ¥å’Œæ§åˆ¶
2. åŠ›ä¼ æ„Ÿå™¨æ•°æ®è¯»å–
3. ç›¸æœºå›¾åƒé‡‡é›†
4. Observationé‡‡é›†ï¼ˆstate + force + imagesï¼‰
5. ç®€å•çš„æœºæ¢°è‡‚æ§åˆ¶æµ‹è¯•

ä½¿ç”¨æ–¹æ³•ï¼š
    python test_jaka_integration.py
    
æ§åˆ¶è¯´æ˜ï¼š
    - ç©ºæ ¼é”®ï¼šæ‰§è¡Œå°å¹…åº¦æµ‹è¯•ç§»åŠ¨
    - 'r': é‡ç½®reference_state
    - 'q': é€€å‡ºæµ‹è¯•
"""

import sys
import time
import numpy as np
import cv2
import torch

# æ·»åŠ lerobotè·¯å¾„
#sys.path.insert(0, '/home/hyx/xiazhi/jaka-Lerobot/lerobot_lab')

from lerobot.common.robot_devices.robots.utils import make_robot


def print_separator(char="=", length=80):
    """æ‰“å°åˆ†éš”çº¿"""
    print(char * length)


def print_section(title):
    """æ‰“å°ç« èŠ‚æ ‡é¢˜"""
    print_separator()
    print(f"  {title}")
    print_separator()


def visualize_observation(obs_dict, frame_count, show_images=True):
    """
    å¯è§†åŒ–è§‚æµ‹æ•°æ®
    
    Args:
        obs_dict: observationå­—å…¸
        frame_count: å¸§è®¡æ•°
        show_images: æ˜¯å¦æ˜¾ç¤ºå›¾åƒçª—å£
    """
    print(f"\n{'='*60}")
    print(f"Frame #{frame_count}")
    print(f"{'='*60}")
    
    # 1. æ‰“å°Stateï¼ˆ7ç»´ï¼šx, y, z, yaw, pitch, roll, gripperï¼‰
    if "observation.state" in obs_dict:
        state = obs_dict["observation.state"]
        print(f"\nğŸ“ State (7D - æœ«ç«¯ä½å§¿):")
        print(f"   Shape: {state.shape}")
        state_np = state.cpu().numpy() if torch.is_tensor(state) else state
        print(f"   x      = {state_np[0]:>8.2f} mm")
        print(f"   y      = {state_np[1]:>8.2f} mm")
        print(f"   z      = {state_np[2]:>8.2f} mm")
        print(f"   yaw    = {state_np[3]:>8.4f} rad ({np.degrees(state_np[3]):>6.2f}Â°)")
        print(f"   pitch  = {state_np[4]:>8.4f} rad ({np.degrees(state_np[4]):>6.2f}Â°)")
        print(f"   roll   = {state_np[5]:>8.4f} rad ({np.degrees(state_np[5]):>6.2f}Â°)")
        print(f"   gripper= {state_np[6]:>8.4f}")
    
    # 2. æ‰“å°Forceï¼ˆå†å²æ•°æ®ï¼š(n_obs_steps, 6)ï¼Œ6ç»´ï¼šfx, fy, fz, mx, my, mzï¼‰
    if "observation.force" in obs_dict:
        force = obs_dict["observation.force"]
        print(f"\nğŸ’ª Force (å†å²åŠ›/åŠ›çŸ©æ•°æ®):")
        print(f"   Shape: {force.shape}")
        force_np = force.cpu().numpy() if torch.is_tensor(force) else force
        
        # è·å–æœ€æ–°å¸§çš„forceæ•°æ®
        if force_np.ndim == 2:
            force_latest = force_np[-1]  # æœ€æ–°å¸§
            print(f"   (æ˜¾ç¤ºæœ€æ–°å¸§ #{force_np.shape[0]}/{ force_np.shape[0]})")
        else:
            force_latest = force_np
        
        print(f"   fx = {force_latest[0]:>8.3f} N")
        print(f"   fy = {force_latest[1]:>8.3f} N")
        print(f"   fz = {force_latest[2]:>8.3f} N")
        print(f"   mx = {force_latest[3]:>8.3f} Nm")
        print(f"   my = {force_latest[4]:>8.3f} Nm")
        print(f"   mz = {force_latest[5]:>8.3f} Nm")
        
        # è®¡ç®—åˆåŠ›
        total_force = np.sqrt(force_latest[0]**2 + force_latest[1]**2 + force_latest[2]**2)
        print(f"   æ€»åŠ› = {total_force:>8.3f} N")
    else:
        print(f"\nâš ï¸  Forceæ•°æ®ä¸å¯ç”¨")
    
    # 3. æ‰“å°Imagesä¿¡æ¯
    image_keys = [k for k in obs_dict.keys() if k.startswith("observation.images")]
    if image_keys:
        print(f"\nğŸ“· Images ({len(image_keys)}ä¸ªç›¸æœº):")
        for key in image_keys:
            img = obs_dict[key]
            img_np = img.cpu().numpy() if torch.is_tensor(img) else img
            cam_name = key.replace("observation.images.", "")
            print(f"   {cam_name:>12}: {img_np.shape} (dtype: {img_np.dtype})")
    
    # 4. æ˜¾ç¤ºå›¾åƒçª—å£ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    if show_images and image_keys:
        for key in image_keys:
            img = obs_dict[key]
            img_np = img.cpu().numpy() if torch.is_tensor(img) else img
            
            # ç¡®ä¿å›¾åƒæ ¼å¼æ­£ç¡®ï¼ˆOpenCVä½¿ç”¨BGRï¼‰
            if img_np.dtype == np.float32 or img_np.dtype == np.float64:
                img_np = (img_np * 255).astype(np.uint8)
            
            # å¦‚æœæ˜¯RGBï¼Œè½¬ä¸ºBGR
            if len(img_np.shape) == 3 and img_np.shape[2] == 3:
                img_display = cv2.cvtColor(img_np, cv2.COLOR_RGB2BGR)
            else:
                img_display = img_np.copy()
            
            # åœ¨å›¾åƒä¸Šå åŠ ä¿¡æ¯
            cam_name = key.replace("observation.images.", "")
            overlay = img_display.copy()
            
            # æ·»åŠ åŠé€æ˜èƒŒæ™¯
            cv2.rectangle(overlay, (5, 5), (400, 150), (0, 0, 0), -1)
            cv2.addWeighted(overlay, 0.3, img_display, 0.7, 0, img_display)
            
            # æ·»åŠ æ–‡æœ¬
            y_offset = 25
            cv2.putText(img_display, f"Frame: {frame_count}", (10, y_offset), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)
            
            if "observation.state" in obs_dict:
                state_np = obs_dict["observation.state"].cpu().numpy() if torch.is_tensor(obs_dict["observation.state"]) else obs_dict["observation.state"]
                y_offset += 20
                cv2.putText(img_display, f"Pos: ({state_np[0]:.1f}, {state_np[1]:.1f}, {state_np[2]:.1f})", 
                           (10, y_offset), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (0, 255, 0), 1)
            
            if "observation.force" in obs_dict:
                force_np = obs_dict["observation.force"].cpu().numpy() if torch.is_tensor(obs_dict["observation.force"]) else obs_dict["observation.force"]
                # è·å–æœ€æ–°å¸§çš„force
                force_latest = force_np[-1] if force_np.ndim == 2 else force_np
                total_force = np.sqrt(force_latest[0]**2 + force_latest[1]**2 + force_latest[2]**2)
                y_offset += 20
                cv2.putText(img_display, f"Force: {total_force:.2f} N", 
                           (10, y_offset), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (0, 255, 255), 1)
            
            # æ˜¾ç¤ºå›¾åƒ
            cv2.imshow(f"JAKA Camera - {cam_name}", img_display)


def test_small_movement(robot, direction="x", delta=5.0):
    """
    æµ‹è¯•å°å¹…åº¦ç§»åŠ¨
    
    Args:
        robot: JakaRobotå®ä¾‹
        direction: ç§»åŠ¨æ–¹å‘ ('x', 'y', 'z', 'yaw', 'pitch', 'roll')
        delta: ç§»åŠ¨é‡ï¼ˆä½ç½®å•ä½mmï¼Œå§¿æ€å•ä½radï¼‰
    """
    print(f"\nğŸ”§ æµ‹è¯•å°å¹…åº¦ç§»åŠ¨: {direction} += {delta}")
    obs = robot.capture_observation()
    reference_state = obs["observation.state"] 
    # åˆ›å»ºactionï¼ˆç›¸å¯¹äºreference_stateçš„å¢é‡ï¼‰
    action = torch.zeros(7, dtype=torch.float32)
    
    direction_map = {
        'x': 0, 'y': 1, 'z': 2,
        'yaw': 3, 'pitch': 4, 'roll': 5,
        'gripper': 6
    }
    
    if direction in direction_map:
        idx = direction_map[direction]
        action[idx] = delta
        
        print(f"   Action: {action.numpy()}")
        
        # å‘é€action
        robot.send_action(action,reference_state)
        print(f"   âœ“ Actionå·²å‘é€")
        
        # ç­‰å¾…è¿åŠ¨å®Œæˆ
        time.sleep(0.5)
        
        # è¯»å–æ–°çŠ¶æ€
        obs = robot.capture_observation()
        new_state = obs["observation.state"].cpu().numpy()
        print(f"   æ–°ä½ç½®: x={new_state[0]:.2f}, y={new_state[1]:.2f}, z={new_state[2]:.2f}")
    else:
        print(f"   âš ï¸ æœªçŸ¥æ–¹å‘: {direction}")


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    
    print_section("JAKAæœºå™¨äººLeRoboté›†æˆæµ‹è¯•")
    print("æµ‹è¯•å†…å®¹ï¼š")
    print("  1. è®¾å¤‡è¿æ¥æµ‹è¯•")
    print("  2. Observationé‡‡é›†æµ‹è¯•")
    print("  3. ç®€å•æ§åˆ¶æµ‹è¯•")
    print("  4. å®æ—¶æ•°æ®æ˜¾ç¤º")
    print("\næ§åˆ¶è¯´æ˜ï¼š")
    print("  ç©ºæ ¼é”® - æ‰§è¡Œæµ‹è¯•ç§»åŠ¨")
    print("  'r'é”®  - é‡ç½®reference_state")
    print("  'q'é”®  - é€€å‡ºæµ‹è¯•")
    print_separator()
    
    input("\næŒ‰Enteré”®å¼€å§‹æµ‹è¯•...")
    
    robot = None
    
    try:
        # ==================== 1. åˆå§‹åŒ–å’Œè¿æ¥ ====================
        print_section("1. åˆå§‹åŒ–JAKAæœºå™¨äºº")
        
        print("åˆ›å»ºrobotå®ä¾‹...")
        robot = make_robot("jaka", inference_time=True)
        print("âœ“ Robotå®ä¾‹åˆ›å»ºæˆåŠŸ")
        
        print(f"\næœºå™¨äººç±»å‹: {robot.robot_type}")
        print(f"æ¨ç†æ¨¡å¼: {robot.inference_time}")
        print(f"é…ç½®çš„ç›¸æœºæ•°é‡: {robot.num_cameras}")
        print(f"æ˜¯å¦å¯ç”¨åŠ›ä¼ æ„Ÿå™¨: {robot.force_sensor is not None}")
        
        print("\næ­£åœ¨è¿æ¥è®¾å¤‡...")
        robot.connect()
        print("âœ“ æ‰€æœ‰è®¾å¤‡è¿æ¥æˆåŠŸ")
        
        # ç­‰å¾…åˆå§‹åŒ–ç¨³å®š
        time.sleep(2)
        
        # ==================== 2. é™æ€æµ‹è¯• ====================
        print_section("2. é™æ€Observationé‡‡é›†æµ‹è¯•ï¼ˆ10å¸§ï¼‰")
        
        for i in range(10):
            print(f"\né‡‡é›†ç¬¬ {i+1}/10 å¸§...")
            obs = robot.capture_observation()
            
            # åªæ‰“å°ç¬¬ä¸€å¸§å’Œæœ€åä¸€å¸§çš„è¯¦ç»†ä¿¡æ¯
            if i == 0 or i == 9:
                visualize_observation(obs, i+1, show_images=(i==0))
            else:
                print(f"  âœ“ å¸§ {i+1} é‡‡é›†æˆåŠŸ")
            
            if i == 0:
                print("\næŒ‰ä»»æ„é”®ç»§ç»­...")
                cv2.waitKey(0)
            
            time.sleep(0.1)
        
        print("\nâœ“ é™æ€é‡‡é›†æµ‹è¯•å®Œæˆ")
        
        # ==================== 3. ç®€å•æ§åˆ¶æµ‹è¯• ====================
        print_section("3. ç®€å•æ§åˆ¶æµ‹è¯•")
        
        print("\nå‡†å¤‡æ‰§è¡Œå°å¹…åº¦ç§»åŠ¨æµ‹è¯•...")
        input("æŒ‰Enteré”®ç»§ç»­...")
        
        # æµ‹è¯•Xæ–¹å‘ç§»åŠ¨
        test_small_movement(robot, direction='x', delta=5.0)
        time.sleep(1)
        
        # æµ‹è¯•Yæ–¹å‘ç§»åŠ¨
        test_small_movement(robot, direction='y', delta=5.0)
        time.sleep(1)
        
        # æµ‹è¯•Zæ–¹å‘ç§»åŠ¨
        test_small_movement(robot, direction='z', delta=3.0)
        time.sleep(1)
        
        print("\nâœ“ ç®€å•æ§åˆ¶æµ‹è¯•å®Œæˆ")
        
        # ==================== 4. å®æ—¶å¾ªç¯æµ‹è¯• ====================
        print_section("4. å®æ—¶æ•°æ®é‡‡é›†ï¼ˆæŒ‰'q'é€€å‡ºï¼‰")
        
        frame_count = 0
        start_time = time.time()
        
        print("\nå¼€å§‹å®æ—¶é‡‡é›†...")
        print("æ§åˆ¶è¯´æ˜ï¼š")
        print("  ç©ºæ ¼é”® - Xæ–¹å‘+5mm")
        print("  'r'é”®  - é‡ç½®reference")
        print("  'q'é”®  - é€€å‡º")
        
        while True:
            frame_count += 1
            
            # é‡‡é›†observation
            obs = robot.capture_observation()
            
            # æ˜¾ç¤ºï¼ˆæ¯5å¸§æ‰“å°ä¸€æ¬¡è¯¦ç»†ä¿¡æ¯ï¼‰
            show_detail = (frame_count % 5 == 0)
            if show_detail:
                visualize_observation(obs, frame_count, show_images=True)
            else:
                # åªæ›´æ–°å›¾åƒçª—å£
                image_keys = [k for k in obs.keys() if k.startswith("observation.images")]
                for key in image_keys:
                    img = obs[key]
                    img_np = img.cpu().numpy() if torch.is_tensor(img) else img
                    if img_np.dtype == np.float32:
                        img_np = (img_np * 255).astype(np.uint8)
                    if len(img_np.shape) == 3 and img_np.shape[2] == 3:
                        img_display = cv2.cvtColor(img_np, cv2.COLOR_RGB2BGR)
                    else:
                        img_display = img_np
                    
                    cam_name = key.replace("observation.images.", "")
                    cv2.imshow(f"JAKA Camera - {cam_name}", img_display)
            
            # å¤„ç†é”®ç›˜è¾“å…¥
            key = cv2.waitKey(1) & 0xFF
            
            if key == ord('q'):
                print("\nç”¨æˆ·è¯·æ±‚é€€å‡º")
                break
            elif key == ord(' '):
                print("\næ‰§è¡Œæµ‹è¯•ç§»åŠ¨...")
                test_small_movement(robot, direction='x', delta=5.0)
            elif key == ord('r'):
                print("\né‡ç½®reference_state...")
                robot.reference_state = None
                print("âœ“ Referenceå·²é‡ç½®ï¼Œä¸‹æ¬¡capture_observationå°†è®°å½•æ–°å‚è€ƒ")
            
            # æ§åˆ¶å¸§ç‡ï¼ˆçº¦30Hzï¼‰
            time.sleep(0.03)
        
        # ç»Ÿè®¡ä¿¡æ¯
        elapsed = time.time() - start_time
        avg_fps = frame_count / elapsed if elapsed > 0 else 0
        print(f"\nç»Ÿè®¡ä¿¡æ¯ï¼š")
        print(f"  æ€»å¸§æ•°: {frame_count}")
        print(f"  è¿è¡Œæ—¶é—´: {elapsed:.2f}s")
        print(f"  å¹³å‡FPS: {avg_fps:.2f}")
        
    except KeyboardInterrupt:
        print("\n\næ”¶åˆ°ä¸­æ–­ä¿¡å·ï¼Œæ­£åœ¨å®‰å…¨é€€å‡º...")
    
    except Exception as e:
        print(f"\nâŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
    
    finally:
        # ==================== 5. æ–­å¼€è¿æ¥ ====================
        print_section("5. æ–­å¼€è¿æ¥")
        
        if robot and robot.is_connected:
            print("æ­£åœ¨æ–­å¼€è®¾å¤‡...")
            robot.disconnect()
            print("âœ“ æ‰€æœ‰è®¾å¤‡å·²æ–­å¼€")
        
        # å…³é—­æ‰€æœ‰OpenCVçª—å£
        cv2.destroyAllWindows()
        
        print_separator()
        print("æµ‹è¯•å®Œæˆï¼")


if __name__ == "__main__":
    main()
